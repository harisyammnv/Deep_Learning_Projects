{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd # used for managing the dataframes\n",
    "import numpy as np # used for managing vectors and matrices\n",
    "from wordcloud import WordCloud,STOPWORDS # for generating word clouds --> to show the most occuring words\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# graphing library\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "py.init_notebook_mode(connected=True)\n",
    "# NLTK libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_pos=pd.DataFrame(columns=['Filename','Text','Polarity'])\n",
    "test_df_pos=pd.DataFrame(columns=['Filename','Text','Polarity'])\n",
    "train_df_neg=pd.DataFrame(columns=['Filename','Text','Polarity'])\n",
    "test_df_neg=pd.DataFrame(columns=['Filename','Text','Polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_dataframe(folder_path):\n",
    "    train_data,train_files=[],[]\n",
    "    test_data,test_files=[],[]\n",
    "    for filename in listdir(folder_path):\n",
    "        if filename.startswith('cv9'):\n",
    "            test_files.append(filename)\n",
    "            test_data.append(load_doc(folder_path+filename))\n",
    "        else:\n",
    "            train_files.append(filename)\n",
    "            train_data.append(load_doc(folder_path+filename))\n",
    "    return train_data,test_data,train_files,test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'movie-polarity-data/pos/'\n",
    "train_data,test_data,train_files,test_files=doc_dataframe(folder_path)\n",
    "\n",
    "train_df_pos['Filename']=train_files\n",
    "train_df_pos['Text']=train_data\n",
    "train_df_pos['Polarity']=1\n",
    "\n",
    "test_df_pos['Filename']=test_files\n",
    "test_df_pos['Text']=test_data\n",
    "test_df_pos['Polarity']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'movie-polarity-data/neg/'\n",
    "train_data,test_data,train_files,test_files=doc_dataframe(folder_path)\n",
    "\n",
    "train_df_neg['Filename']=train_files\n",
    "train_df_neg['Text']=train_data\n",
    "train_df_neg['Polarity']=0\n",
    "\n",
    "test_df_neg['Filename']=test_files\n",
    "test_df_neg['Text']=test_data\n",
    "test_df_neg['Polarity']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.concat([train_df_pos,train_df_neg])\n",
    "test_df=pd.concat([test_df_pos,test_df_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function cleans the data and replace numbers by a common token: NUM. This function is inspired in kim's work\n",
    "# at https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "import re\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),:!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\":\", \" : \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = string.strip().lower()\n",
    "    string = re.sub(r\"[0-9]{2,}\", \"NUM\", string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Cleaned_Text\"]=train_df[\"Text\"].apply(clean_str)\n",
    "test_df[\"Cleaned_Text\"]=test_df[\"Text\"].apply(clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Text</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cv087_1989.txt</td>\n",
       "      <td>many people dislike french films for their lac...</td>\n",
       "      <td>1</td>\n",
       "      <td>many people dislike french films for their lac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv525_16122.txt</td>\n",
       "      <td>\" take a number , fill out a form , and wait ...</td>\n",
       "      <td>1</td>\n",
       "      <td>take a number , fill out a form , and wait you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv116_28942.txt</td>\n",
       "      <td>capsule : a short punchy action sequel to the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>capsule : a short punchy action sequel to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv130_17083.txt</td>\n",
       "      <td>while watching wes anderson's rushmore , it ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>while watching wes anderson 's rushmore , it m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cv567_29611.txt</td>\n",
       "      <td>plot : a peculiar french girl grows up lonely ...</td>\n",
       "      <td>1</td>\n",
       "      <td>plot : a peculiar french girl grows up lonely ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Filename                                               Text  \\\n",
       "0   cv087_1989.txt  many people dislike french films for their lac...   \n",
       "1  cv525_16122.txt   \" take a number , fill out a form , and wait ...   \n",
       "2  cv116_28942.txt  capsule : a short punchy action sequel to the ...   \n",
       "3  cv130_17083.txt  while watching wes anderson's rushmore , it ma...   \n",
       "4  cv567_29611.txt  plot : a peculiar french girl grows up lonely ...   \n",
       "\n",
       "   Polarity                                       Cleaned_Text  \n",
       "0         1  many people dislike french films for their lac...  \n",
       "1         1  take a number , fill out a form , and wait you...  \n",
       "2         1  capsule : a short punchy action sequel to the ...  \n",
       "3         1  while watching wes anderson 's rushmore , it m...  \n",
       "4         1  plot : a peculiar french girl grows up lonely ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary has 37054 words\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# this function will help in creating the tokenized Input Tensor \n",
    "vectorizer = CountVectorizer(stop_words='english',lowercase=False)\n",
    "\n",
    "# fit the data\n",
    "cleaned_text = vectorizer.fit(train_df[\"Cleaned_Text\"])\n",
    "\n",
    "# Get the vocabulary\n",
    "vocabulary = cleaned_text.vocabulary_\n",
    "\n",
    "# vocabulary is a dictionary with keys as the words and the values as frequencies\n",
    "words = set(vocabulary.keys())\n",
    "\n",
    "# Size of vocabulary\n",
    "size_vocab = len(vocabulary)\n",
    "print (\"The vocabulary has {} words\".format(size_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest sentence has 1292 tokens\n"
     ]
    }
   ],
   "source": [
    "# Get the data cleaned and transform words in numerical tokens.\n",
    "text = train_df['Cleaned_Text'].tolist()\n",
    "text = [string.split() for string in text]\n",
    "# tokens is a list of lists. Each list inside tokens represents a sentence\n",
    "tokens = []\n",
    "for sentence in text:\n",
    "    dummy = []\n",
    "    for word in sentence:\n",
    "        if word in words:\n",
    "            dummy.append(vocabulary[word] + 1) # Sum 1 cause I want to leave the token 0 for padding in LSTM.\n",
    "    tokens.append(dummy)\n",
    "    \n",
    "# Maximum length of a sentence in the data\n",
    "max_len = max([len(sentence) for sentence in tokens])\n",
    "print (\"The largest sentence has {} tokens\".format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tokens\n",
    "Y=train_df[['Polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,Y_train,Y_val=train_test_split(X,Y,test_size=0.1,random_state=0,stratify=Y.Polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harisyam_bphc/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence padding\n",
    "trainX = pad_sequences(X_train, maxlen=max_len, padding='post') # max_len was found some cells above\n",
    "validationX = pad_sequences(X_val, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1620, 1292)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers.core import Activation,Dense,Dropout,SpatialDropout1D\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE=100\n",
    "HIDDEN_LAYER_SIZE=64\n",
    "BATCH_SIZE=32\n",
    "NUM_EPOCHS=10\n",
    "DROPOUT=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(size_vocab+1,EMBEDDING_SIZE,input_length=max_len))\n",
    "model.add(SpatialDropout1D(DROPOUT))\n",
    "model.add(Bidirectional(LSTM(HIDDEN_LAYER_SIZE,dropout=DROPOUT,recurrent_dropout=DROPOUT)))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1292, 100)         3705500   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 1292, 100)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,790,238\n",
      "Trainable params: 3,790,238\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename=\"model_movie_reviews.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y=pd.get_dummies(Y_train,columns=['Polarity'])\n",
    "val_Y=pd.get_dummies(Y_val,columns=['Polarity'])\n",
    "train_Y=train_Y.values\n",
    "val_Y=val_Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1620 samples, validate on 180 samples\n",
      "Epoch 1/10\n",
      "1620/1620 [==============================] - 262s 161ms/step - loss: 0.6936 - acc: 0.5071 - val_loss: 0.6913 - val_acc: 0.6028\n",
      "Epoch 2/10\n",
      "1620/1620 [==============================] - 264s 163ms/step - loss: 0.6558 - acc: 0.6892 - val_loss: 0.6226 - val_acc: 0.6861\n",
      "Epoch 3/10\n",
      "1620/1620 [==============================] - 263s 162ms/step - loss: 0.4865 - acc: 0.8340 - val_loss: 0.5558 - val_acc: 0.7389\n",
      "Epoch 4/10\n",
      "1620/1620 [==============================] - 260s 161ms/step - loss: 0.3147 - acc: 0.9049 - val_loss: 0.5382 - val_acc: 0.7833\n",
      "Epoch 5/10\n",
      "1620/1620 [==============================] - 264s 163ms/step - loss: 0.2161 - acc: 0.9386 - val_loss: 0.6211 - val_acc: 0.7194\n",
      "Epoch 6/10\n",
      "1620/1620 [==============================] - 265s 164ms/step - loss: 0.1313 - acc: 0.9636 - val_loss: 0.7176 - val_acc: 0.6833\n"
     ]
    }
   ],
   "source": [
    "checkpoint=ModelCheckpoint(model_filename, monitor='val_acc', verbose=0, save_best_only=False, mode='auto', period=1)\n",
    "early=EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "history=model.fit(trainX, train_Y,batch_size=32,epochs=NUM_EPOCHS,validation_data=(validationX, val_Y),callbacks=[checkpoint,early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data cleaned and transform words in numerical tokens.\n",
    "text = test_df['Cleaned_Text'].tolist()\n",
    "text = [string.split() for string in text]\n",
    "# tokens is a list of lists. Each list inside tokens represents a sentence\n",
    "tokens = []\n",
    "for sentence in text:\n",
    "    dummy = []\n",
    "    for word in sentence:\n",
    "        if word in words:\n",
    "            dummy.append(vocabulary[word] + 1) # Sum 1 cause I want to leave the token 0 for padding in LSTM.\n",
    "    tokens.append(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=tokens\n",
    "Xtest = pad_sequences(X_test, maxlen=1292, padding='post') # max_len was found some cells above\n",
    "Ytest=test_df[['Polarity']]\n",
    "Ytest=pd.get_dummies(Ytest,columns=['Polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 64.500000\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(Xtest, Ytest.values, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
